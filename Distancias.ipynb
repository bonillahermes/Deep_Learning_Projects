{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonillahermes/Deep_Learning_Projects/blob/main/Distancias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hermes Yate Bonilla\n",
        "**Data Scientist**\n",
        "---\n",
        "\n",
        "**Contact:**\n",
        "- **Email:** [bonillahermes@gmail.com](mailto:bonillahermes@gmail.com)\n",
        "- **LinkedIn:** [linkedin.com/in/bonillahermes](https://www.linkedin.com/in/bonillahermes/)\n",
        "- **GitHub:** [github.com/bonillahermes](https://github.com/bonillahermes)\n",
        "- **Webpage:** [bonillahermes.com](https://bonillahermes.com/)\n",
        "---"
      ],
      "metadata": {
        "id": "B6QfdL3jLeHk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tHLams-76As"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.patches as mpatches\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim #Optim module of Pytorch provides Optimization algorithms for training NN\n",
        "import rasterio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import io\n",
        "import gc  # garbage collector\n",
        "\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import Patch\n",
        "import PIL\n",
        "\n",
        "from Junglenet import Junglenet\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "data_folder = os.path.expanduser('/export/data/s7rocast/AnthroProtect/data/anthroprotect')\n",
        "\n",
        "\n",
        "# Activation Space\n",
        "\n",
        "class APDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split, data_folder, device):\n",
        "\n",
        "        # file folder\n",
        "        self.file_folder = os.path.join(data_folder, 'tiles/s2')\n",
        "\n",
        "        # df\n",
        "        csv_file = os.path.join(data_folder, 'infos.csv')\n",
        "        df = pd.read_csv(csv_file)\n",
        "        df = df[df['datasplit'] == split]\n",
        "\n",
        "\n",
        "\n",
        "        self.files = df['file'].to_list()\n",
        "        self.targets = df['label'].to_list()\n",
        "\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get x\n",
        "        file = self.files[index]\n",
        "        file = os.path.join(self.file_folder, file)\n",
        "\n",
        "        with rasterio.open(file) as src:\n",
        "            x = src.read()\n",
        "\n",
        "        # transform x\n",
        "        x = x.astype(np.float32)\n",
        "        x = torch.tensor(x)\n",
        "        x /= 10000\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # get y\n",
        "        y = self.targets[index]\n",
        "\n",
        "        # transform y\n",
        "        y = torch.tensor(y, dtype=torch.float32)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        return {'x': x, 'y': y, 'file': file}\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.files)\n",
        "\n",
        "# all datasets\n",
        "train_dataset = APDataset(split='train', data_folder=data_folder, device=device)\n",
        "val_dataset = APDataset(split='val', data_folder=data_folder, device=device)\n",
        "test_dataset = APDataset(split='test', data_folder=data_folder, device=device)\n",
        "\n",
        "\n",
        "print(val_dataset[0])\n",
        "\n",
        "model = Junglenet(\n",
        "    in_channels=10,\n",
        "    n_unet_maps=3,\n",
        "    n_classes=1,\n",
        "\n",
        "    unet_base_channels=32,\n",
        "    double_conv=False,\n",
        "    batch_norm=True,\n",
        "    unet_mode='bilinear',\n",
        "    unet_activation=nn.Tanh(),\n",
        "\n",
        "    dropout=None,\n",
        "    final_activation=nn.Sigmoid(),\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Load Model\n",
        "\n",
        "model.load_state_dict(torch.load('/export/data/s7rocast/RepositoryThesis/model/junglenet_epoch_88.pth'))\n",
        "\n",
        "# Land cover\n",
        "class APDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split, data_folder, device):\n",
        "\n",
        "        # file folder\n",
        "        self.file_folder = os.path.join(data_folder, 'tiles/esa_lc/lc_esawc')\n",
        "\n",
        "        # df\n",
        "        csv_file = os.path.join(data_folder, 'infos.csv')\n",
        "        df = pd.read_csv(csv_file)\n",
        "        df = df[df['datasplit'] == split]\n",
        "\n",
        "\n",
        "\n",
        "        self.files = df['file'].to_list()\n",
        "        self.targets = df['label'].to_list()\n",
        "\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get x\n",
        "        file = self.files[index]\n",
        "        file = os.path.join(self.file_folder, file)\n",
        "\n",
        "        with rasterio.open(file) as src:\n",
        "            x = src.read()\n",
        "\n",
        "        # transform x\n",
        "        x = x.astype(np.float32)\n",
        "        x = torch.tensor(x)\n",
        "        #x /= 10000\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # get y\n",
        "        y = self.targets[index]\n",
        "\n",
        "        # transform y\n",
        "        y = torch.tensor(y, dtype=torch.float32)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        #return {'x': x, 'y': y, 'file': file}\n",
        "        return {'x': x}\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.files)\n",
        "\n",
        "# all datasets\n",
        "esa_train_dataset = APDataset(split='train', data_folder=data_folder, device=device)\n",
        "esa_val_dataset = APDataset(split='val', data_folder=data_folder, device=device)\n",
        "esa_test_dataset = APDataset(split='test', data_folder=data_folder, device=device)\n",
        "\n",
        "\n",
        "print(esa_val_dataset[0])\n",
        "\n",
        "\n",
        "# Obtain Activation and Land cover vectors for a set of random images\n",
        "def get_acts_and_lc_l(indices):\n",
        "    act_vectors_list = []\n",
        "    lc_vectors_list = []\n",
        "\n",
        "    for index in indices:\n",
        "        x = train_dataset[index]['x']\n",
        "        lc_map = esa_train_dataset[index]['x'][0].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # model.eval() ask if it is needed\n",
        "        with torch.no_grad():\n",
        "            act_map = model.unet(x.unsqueeze(0))\n",
        "\n",
        "        n_channels = act_map.shape[1]\n",
        "        act_vectors = act_map.permute(0, 2, 3, 1).reshape(-1, n_channels).cpu()\n",
        "        act_vectors_list.append(act_vectors)\n",
        "\n",
        "        n_channels = lc_map.shape[1]\n",
        "        lc_vectors = lc_map.permute(0, 2, 3, 1).reshape(-1, n_channels).cpu()\n",
        "        lc_vectors_list.append(lc_vectors)\n",
        "\n",
        "    # Concatenate the lists to get the final tensors\n",
        "    act_vectors = torch.cat(act_vectors_list, dim=0)\n",
        "    lc_vectors = torch.cat(lc_vectors_list, dim=0)\n",
        "\n",
        "    return act_vectors, lc_vectors\n",
        "\n",
        "# Example usage with a random set of indices\n",
        "total_images = len(train_dataset)  # Assuming train_dataset is a list or similar structure\n",
        "num_images_to_select = 200  # Change this to the desired number of random images\n",
        "\n",
        "# Randomly select indices\n",
        "random_indices = random.sample(range(total_images), num_images_to_select)\n",
        "\n",
        "# Get Activation and Land cover vectors for the randomly selected indices\n",
        "act_vectors, lc_vectors = get_acts_and_lc_l(random_indices)\n",
        "\n",
        "print(lc_vectors.shape)\n",
        "print(act_vectors.shape)\n",
        "\n",
        "colors=['#006400', '#ffbb22', '#ffff4c', '#f096ff', '#fa0000', '#b4b4b4', '#f0f0f0', '#0064c8', '#0096a0', '#00cf75', '#fae6a0']\n",
        "\n",
        "classes = [10, 20, 30, 40, 50, 60,70, 80, 90, 95, 100]\n",
        "\n",
        "classes_weights = [49, 99.6, 78, 87, 150, 99, 99.5, 95, 98.5, 100, 95]\n",
        "#classes_weights = [40, 70, 60, 500, 2000, 78, 79, 75, 78, 100, 75]\n",
        "\n",
        "def get_cmap(colors=None, classes=None, epsilon: float = 1e-2, framing_color: str = 'white', plot: bool = False):\n",
        "    \"\"\"\n",
        "    Creates a colormap from given colors and classes. E.g. if you have classes [11, 12, 21, 31, ...] with the\n",
        "    following colors ['blue', 'red', 'green', 'yellow', ...] you can give these two lists as colors and classes.\n",
        "\n",
        "    :param colors: list of colors, e.g. ['blue', 'red', 'green', 'yellow', ...]\n",
        "    :param classes: list of classes, e.g. [11, 12, 21, 31, ...]\n",
        "    :param epsilon: to correctly plot colors accoring to numbers, the numbers must be a bit lower; this i.a. can be\n",
        "        controlled with epsilon\n",
        "    :param framing_color: all numbers lower or higher that given min/max bound will be plotted in framing_color;\n",
        "        if None, lower or higher number will be plotted in lowest or highest given color\n",
        "    :param plot: bool if colormap shall be plotted for test reasons\n",
        "    :return: (cmap, norm) colormap and normalization\n",
        "    \"\"\"\n",
        "\n",
        "    bounds = np.array(classes)\n",
        "    bounds = np.hstack([bounds, bounds.max() + 2 * epsilon])  # there must be a bound larger then given max bound\n",
        "    bounds = np.array(bounds) - epsilon  # all bounds should be a little smaller than given, if categories are plotted\n",
        "\n",
        "    if framing_color is not None:\n",
        "\n",
        "        colors = [framing_color] + list(colors) + [framing_color]\n",
        "        bounds = np.array([bounds.min() - epsilon] + list(bounds) + [bounds.max() + epsilon])\n",
        "\n",
        "    cmap = mpl.colors.ListedColormap(colors=colors)\n",
        "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    if plot:\n",
        "        classes = np.array(classes)\n",
        "        array = [np.hstack([classes.min() - 1, classes, classes.max() + 1])]\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(array, cmap=cmap, norm=norm)\n",
        "        ax.axis(False)\n",
        "        fig.colorbar(im, ticks=classes, orientation='horizontal')\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return cmap, norm\n",
        "cmap, norm = get_cmap(colors,classes)\n",
        "\n",
        "n_pixeles = 1000000\n",
        "cantidad_de_pixeles = act_vectors.shape[0]\n",
        "# Define class weights\n",
        "class_weights = {class_val: weight for class_val, weight in zip(classes, classes_weights)}\n",
        "\n",
        "# Randomly select pixels with weighted randomization\n",
        "lc_values = [float(class_val.item()) for class_val in lc_vectors[:, 0]]\n",
        "class_weights_sum = sum(class_weights[class_val] for class_val in lc_values)\n",
        "normalized_probabilities = [class_weights[class_val] / class_weights_sum for class_val in lc_values]\n",
        "\n",
        "weighted_random_indices = np.random.choice(cantidad_de_pixeles, size=n_pixeles, p=normalized_probabilities)\n",
        "\n",
        "act_vectors_filtered = act_vectors[weighted_random_indices]\n",
        "lc_vectors_filtered = lc_vectors[weighted_random_indices]\n",
        "\n",
        "class_names_mapping = {\n",
        "    10.0: 'Tree cover',\n",
        "    20.0: 'Shrubland',\n",
        "    30.0: 'Grassland',\n",
        "    40.0: 'Cropland',\n",
        "    50.0: 'Built-up',\n",
        "    60.0: 'Bare / sparse vegetation',\n",
        "    70.0: 'Snow and ice',\n",
        "    80.0: 'Permanent water bodies',\n",
        "    90.0: 'Herbaceous wetland',\n",
        "    95.0: 'Mangroves',\n",
        "    100.0: 'Moss and lichen'\n",
        "}\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
        "gm = GaussianMixture(n_components=2, random_state=0).fit(X)\n",
        "gm.means_\n",
        "array([[10.,  2.],\n",
        "       [ 1.,  2.]])\n",
        "gm.predict([[0, 0], [12, 3]])\n",
        "array([1, 0])\n",
        "\n",
        "# Dimensionality Reduction\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "act_vectors_np = act_vectors_filtered.numpy()\n",
        "scaler = StandardScaler()\n",
        "act_vectors_std = scaler.fit_transform(act_vectors_np)\n",
        "pca = PCA(n_components=2)\n",
        "act_vectors_pca = pca.fit_transform(act_vectors_std)\n",
        "print(act_vectors_pca.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducción de dimensionalidad y GMM"
      ],
      "metadata": {
        "id": "G0vkNm1Y-4g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bYjuu74VCH_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):  # Imprimir los primeros 5 pares de vectores y etiquetas\n",
        "    print(f\"Vector de Activación {i}: {act_vectors_filtered[i]}\")\n",
        "    print(f\"Etiqueta de Cobertura de Tierra {i}: {lc_vectors_filtered[i]}\")\n"
      ],
      "metadata": {
        "id": "MrcrL5dQCDdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducir la dimensionalidad a 2 componentes con PCA\n",
        "pca = PCA(n_components=2)\n",
        "act_vectors_reduced = pca.fit_transform(act_vectors_filtered)\n",
        "print(act_vectors_reduced.shape)\n"
      ],
      "metadata": {
        "id": "zdZ4nN2v4WRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criterio Bayesiano: Evalua cuantos clusters existen en el modelo. Numero de distribuciones en mis datos. ajuste de dimensiones"
      ],
      "metadata": {
        "id": "7PKA5lB8iczP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "act_vectors_np = act_vectors_filtered.numpy()\n",
        "\n",
        "# Definir el rango del número de componentes\n",
        "n_max_components = 10  # Puedes ajustar este valor"
      ],
      "metadata": {
        "id": "3cVrMIc-jBO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BIC_scores = []\n",
        "for i in range(1, n_max_components + 1):\n",
        "    gmm = GaussianMixture(n_components=i, random_state=0).fit(act_vectors_np)\n",
        "    BIC_scores.append(gmm.bic(act_vectors_np))\n",
        "\n",
        "# Encontrar el número óptimo de componentes\n",
        "optimal_n_components = np.argmin(BIC_scores) + 1\n",
        "print(f\"El número óptimo de componentes es: {optimal_n_components}\")\n",
        "\n",
        "# Graficar los BIC scores para visualizar el 'codo'\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, n_max_components + 1), BIC_scores, marker='o')\n",
        "plt.title('GMM BIC Score por Número de Componentes')\n",
        "plt.xlabel('Número de Componentes')\n",
        "plt.ylabel('BIC Score')\n",
        "plt.xticks(range(1, n_max_components + 1))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "PKpNQYqqJmIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AIC_scores = []\n",
        "for i in range(1, n_max_components + 1):\n",
        "    gmm = GaussianMixture(n_components=i, random_state=0).fit(act_vectors_np)\n",
        "    AIC_scores.append(gmm.aic(act_vectors_np))\n",
        "\n",
        "# Encontrar el número óptimo de componentes\n",
        "optimal_n_components = np.argmin(AIC_scores) + 1\n",
        "print(f\"El número óptimo de componentes es: {optimal_n_components}\")\n",
        "\n",
        "# Graficar los AIC scores para visualizar el 'codo'\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, n_max_components + 1), AIC_scores, marker='o')\n",
        "plt.title('GMM AIC Score por Número de Componentes')\n",
        "plt.xlabel('Número de Componentes')\n",
        "plt.ylabel('AIC Score')\n",
        "plt.xticks(range(1, n_max_components + 1))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "R3zjf1d3J-7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar Gaussian Mixture Model\n",
        "n_components = 10  # Ajustar este rango según sea necesario\n",
        "gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
        "gmm.fit(act_vectors_reduced)"
      ],
      "metadata": {
        "id": "QH1WxP9vJzKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clasificar cada punto en uno de los clusters\n",
        "labels = gmm.predict(act_vectors_reduced)"
      ],
      "metadata": {
        "id": "Im6gYdGMF3f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar los clusters\n",
        "plt.scatter(act_vectors_reduced[:, 0], act_vectors_reduced[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Clusters GMM')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.colorbar(label='Etiqueta del Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ir-TnBYPF7AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bondad de ajuste del GMM"
      ],
      "metadata": {
        "id": "0eW2WGek4rNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_likelihood = gmm.score(act_vectors_reduced)\n",
        "print(\"Log Likelihood: \", log_likelihood)"
      ],
      "metadata": {
        "id": "wy_1SXHQ4qUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_avg = silhouette_score(act_vectors_reduced, labels)\n",
        "print(\"Silhouette Score: \", silhouette_avg)\n"
      ],
      "metadata": {
        "id": "6miUg_sy49rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot de clasificación"
      ],
      "metadata": {
        "id": "o7v3AtRK5BjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraer las etiquetas de capa terrestre\n",
        "land_cover_labels = lc_vectors_filtered[:, 0]  # Modificar según etiquetado\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Crear un scatter plot donde cada punto es coloreado según su tipo de capa terrestre\n",
        "scatter = plt.scatter(act_vectors_reduced[:, 0], act_vectors_reduced[:, 1], c=land_cover_labels, cmap='viridis')\n",
        "\n",
        "# Crear una leyenda con los tipos de capa terrestre\n",
        "# Reemplaza esto con tus propios nombres de capa terrestre\n",
        "class_names_mapping = {\n",
        "    10.0: 'Tree cover',\n",
        "    20.0: 'Shrubland',\n",
        "    30.0: 'Grassland',\n",
        "    40.0: 'Cropland',\n",
        "    50.0: 'Built-up',\n",
        "    60.0: 'Bare / sparse vegetation',\n",
        "    70.0: 'Snow and ice',\n",
        "    80.0: 'Permanent water bodies',\n",
        "    90.0: 'Herbaceous wetland',\n",
        "    95.0: 'Mangroves',\n",
        "    100.0: 'Moss and lichen'\n",
        "}\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=class_names_mapping.values, loc='best')\n",
        "\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('Visualización por Tipo de Capa Terrestre')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LyUH33HR-h_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Varianza o Desviación Estándar dentro de cada Cluster"
      ],
      "metadata": {
        "id": "mvQ6pGHEGBZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dispersions = []\n",
        "\n",
        "for i in range(gmm.n_components):\n",
        "    cluster_points = act_vectors_reduced[labels == i]\n",
        "    dispersion = np.var(cluster_points, axis=0)\n",
        "    cluster_dispersions.append(dispersion)\n",
        "\n",
        "# cluster_dispersions ahora contiene la varianza para cada cluster\n"
      ],
      "metadata": {
        "id": "q5LMpwUlFroE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distancia Media al Centroide"
      ],
      "metadata": {
        "id": "RkCD_mmbGeB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "centroids = gmm.means_\n",
        "mean_distances = []\n",
        "\n",
        "for i in range(gmm.n_components):\n",
        "    cluster_points = act_vectors_reduced[labels == i]\n",
        "    centroid = centroids[i]\n",
        "    distances = np.sqrt(np.sum((cluster_points - centroid) ** 2, axis=1))\n",
        "    mean_distance = np.mean(distances)\n",
        "    mean_distances.append(mean_distance)\n",
        "\n",
        "# mean_distances ahora contiene la distancia media para cada cluster\n"
      ],
      "metadata": {
        "id": "sabN23g2Gl9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KL Divergencia"
      ],
      "metadata": {
        "id": "yNQ-LqGRD0QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "from scipy.integrate import quad\n",
        "\n",
        "# Crear Data como un conjunto de datos unidimensional\n",
        "data = act_vectors_filtered.numpy()[:, 0]  # Modifica según requerimientos\n",
        "\n",
        "# Estimación de la densidad de los datos usando KDE\n",
        "kde = gaussian_kde(data)\n",
        "\n",
        "# Función de densidad de probabilidad del GMM\n",
        "gmm_pdf = lambda x: np.exp(gmm.score_samples(x.reshape(-1, 1)))\n",
        "\n",
        "# Función para calcular la divergencia KL\n",
        "def kl_divergence(p, q, start=-10, end=10):\n",
        "    return quad(lambda x: p(x) * np.log(p(x) / q(x)), start, end)[0]\n",
        "\n",
        "# Calcular la divergencia KL\n",
        "kl_div = kl_divergence(kde, gmm_pdf)\n",
        "print(\"KL Divergence: \", kl_div)\n"
      ],
      "metadata": {
        "id": "xOoa7xCKD260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "from scipy.integrate import quad\n",
        "\n",
        "data = act_vectors_filtered.numpy()[:, 0]\n",
        "\n",
        "# Estimación de la densidad de los datos usando KDE\n",
        "kde = gaussian_kde(data)\n",
        "\n",
        "# Modificar la función lambda para manejar entradas escalares\n",
        "gmm_pdf = lambda x: np.exp(gmm.score_samples(np.atleast_1d(x).reshape(-1, 1)))\n",
        "\n",
        "# Función para calcular la divergencia KL\n",
        "def kl_divergence(p, q, start=-10, end=10):\n",
        "    return quad(lambda x: p(x) * np.log(p(x) / q(x)), start, end)[0]\n",
        "\n",
        "# Calcular la divergencia KL\n",
        "kl_div = kl_divergence(kde, gmm_pdf)\n",
        "print(\"KL Divergence: \", kl_div)\n"
      ],
      "metadata": {
        "id": "2YbiIHCvf9ir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}